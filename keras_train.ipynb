{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import params\n",
    "import utils\n",
    "import os\n",
    "import tensorflow_probability as tfp\n",
    "import data_preparation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "tfd = tfp.distributions\n",
    "keras = tf.keras\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "NUM_CLASSES = len(params.brand_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class constrain_conv(tf.keras.models.Model, tf.keras.callbacks.Callback):\n",
    "    def __init__(self, model):\n",
    "        super(constrain_conv, self).__init__()\n",
    "        self.layer = model.layers[0]\n",
    "        self.pre_weights = None\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        weights = self.layer.get_weights()[0]\n",
    "        bias = self.layer.get_weights()[1]\n",
    "        if self.pre_weights is None or np.all(self.pre_weights != weights):\n",
    "            weights = weights*10000\n",
    "            weights[2, 2, :, :] = 0\n",
    "            s = np.sum(weights, axis=(0,1))\n",
    "            for i in range(3):\n",
    "                weights[:, :, 0, i] /= s[0, i]\n",
    "            weights[2, 2, :, :] = -1\n",
    "            self.pre_weights = weights\n",
    "        self.layer.set_weights([weights, bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2359/2359 [00:00<00:00, 267277.97it/s]\n"
     ]
    }
   ],
   "source": [
    "data_preparation.collect_split_extract()\n",
    "\n",
    "train_size = 0\n",
    "val_size = 0\n",
    "num_images_per_class = []\n",
    "class_weight = {}\n",
    "for m in params.brand_models:\n",
    "    num_images = len(os.listdir(os.path.join(params.patches_dir, 'train', m)))\n",
    "    num_images_per_class.append(num_images)\n",
    "    train_size += num_images\n",
    "    val_size += len(os.listdir(os.path.join(params.patches_dir, 'val', m)))\n",
    "    \n",
    "num_batches = (train_size + params.BATCH_SIZE - 1) // params.BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(params.brand_models)):\n",
    "    class_weight[n] = (1 / num_images_per_class[n])*(train_size)/2.0\n",
    "    print('Weight for class {}: {:.2f}'.format(n, class_weight[n]))\n",
    "\n",
    "def _posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
    "    \"\"\"Posterior function for variational layer.\"\"\"\n",
    "    n = kernel_size + bias_size\n",
    "    c = np.log(np.expm1(1e-5))\n",
    "    variable_layer = tfp.layers.VariableLayer(\n",
    "      2 * n, dtype=dtype,\n",
    "      initializer=tfp.layers.BlockwiseInitializer([\n",
    "          keras.initializers.TruncatedNormal(mean=0., stddev=.05, seed=None),\n",
    "          keras.initializers.Constant(np.log(np.expm1(1e-5)))], sizes=[n, n]))\n",
    "\n",
    "    def distribution_fn(t):\n",
    "        scale = 1e-5 + tf.nn.softplus(c + t[Ellipsis, n:])\n",
    "        return tfd.Independent(tfd.Normal(loc=t[Ellipsis, :n], scale=scale),\n",
    "                           reinterpreted_batch_ndims=1)\n",
    "    distribution_layer = tfp.layers.DistributionLambda(distribution_fn)\n",
    "    return tf.keras.Sequential([variable_layer, distribution_layer])\n",
    "\n",
    "\n",
    "def _make_prior_fn(kernel_size, bias_size=0, dtype=None):\n",
    "    del dtype  # TODO(yovadia): Figure out what to do with this.\n",
    "    loc = tf.zeros(kernel_size + bias_size)\n",
    "    def distribution_fn(_):\n",
    "        return tfd.Independent(tfd.Normal(loc=loc, scale=1),\n",
    "                           reinterpreted_batch_ndims=1)\n",
    "    return distribution_fn\n",
    "\n",
    "\n",
    "def make_divergence_fn_for_empirical_bayes(std_prior_scale, examples_per_epoch):\n",
    "    def divergence_fn(q, p, _):\n",
    "        log_probs = tfd.LogNormal(0., std_prior_scale).log_prob(p.stddev())\n",
    "        out = tfd.kl_divergence(q, p) - tf.reduce_sum(log_probs)\n",
    "        return out / examples_per_epoch\n",
    "    return divergence_fn\n",
    "\n",
    "\n",
    "def make_prior_fn_for_empirical_bayes(init_scale_mean=-1, init_scale_std=0.1):\n",
    "    \"\"\"Returns a prior function with stateful parameters for EB models.\"\"\"\n",
    "    def prior_fn(dtype, shape, name, _, add_variable_fn):\n",
    "        \"\"\"A prior for the variational layers.\"\"\"\n",
    "        untransformed_scale = add_variable_fn(\n",
    "            name=name + '_untransformed_scale',\n",
    "            shape=(1,),\n",
    "            initializer=tf.compat.v1.initializers.random_normal(\n",
    "                mean=init_scale_mean, stddev=init_scale_std),\n",
    "            dtype=dtype,\n",
    "            trainable=False)\n",
    "        loc = add_variable_fn(\n",
    "            name=name + '_loc',\n",
    "            initializer=keras.initializers.Zeros(),\n",
    "            shape=shape,\n",
    "            dtype=dtype,\n",
    "            trainable=True)\n",
    "        # ??? why 1e-6 ???\n",
    "        scale = 1e-6 + tf.nn.softplus(untransformed_scale)\n",
    "        dist = tfd.Normal(loc=loc, scale=scale)\n",
    "        batch_ndims = tf.size(input=dist.batch_shape_tensor())\n",
    "        return tfd.Independent(dist, reinterpreted_batch_ndims=batch_ndims)\n",
    "    return prior_fn\n",
    "\n",
    "init_prior_scale_mean=-1.9994,\n",
    "init_prior_scale_std=-0.30840,\n",
    "std_prior_scale=3.4210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "eb_prior_fn = make_prior_fn_for_empirical_bayes(\n",
    "              init_prior_scale_mean, init_prior_scale_std)\n",
    "\n",
    "divergence_fn = make_divergence_fn_for_empirical_bayes(\n",
    "        std_prior_scale, train_size)\n",
    "\n",
    "kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /  # pylint: disable=g-long-lambda\n",
    "                        tf.cast(train_size, dtype=tf.float32))\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(3, (5, 5), \n",
    "                           padding='same'),\n",
    "    tfp.layers.Convolution2DFlipout(96, \n",
    "        kernel_size=7, strides=2, padding='SAME', \n",
    "        kernel_prior_fn=eb_prior_fn,\n",
    "        kernel_divergence_fn=divergence_fn,\n",
    "        activation=tf.nn.selu),\n",
    "    tf.keras.layers.MaxPool2D(\n",
    "        pool_size=[3, 3], strides=2,\n",
    "        padding='SAME'),\n",
    "    tfp.layers.Convolution2DFlipout(\n",
    "        64, kernel_size=5, strides=1,\n",
    "        padding='SAME', \n",
    "        kernel_prior_fn=eb_prior_fn,\n",
    "        kernel_divergence_fn=divergence_fn,\n",
    "        activation=tf.nn.selu),\n",
    "    tf.keras.layers.MaxPool2D(\n",
    "        pool_size=[3, 3], strides=1,\n",
    "        padding='SAME'),\n",
    "    tfp.layers.Convolution2DFlipout(\n",
    "        128, kernel_size=1, \n",
    "        strides=1, padding='SAME',\n",
    "        kernel_prior_fn=eb_prior_fn,\n",
    "        kernel_divergence_fn=divergence_fn,\n",
    "        activation=tf.nn.selu),\n",
    "    tf.keras.layers.MaxPool2D(\n",
    "        pool_size=[3, 3], strides=2,\n",
    "        padding='SAME'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "#     tfp.layers.DenseFlipout(\n",
    "#         50, kernel_divergence_fn=kl_divergence_function,\n",
    "#         activation=tf.nn.selu),\n",
    "    tfp.layers.DenseFlipout(\n",
    "        NUM_CLASSES, \n",
    "        kernel_prior_fn=eb_prior_fn,\n",
    "        kernel_divergence_fn=divergence_fn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = (tf.data.Dataset.list_files(params.patches_dir + '/train/*/*')\n",
    "    .shuffle(buffer_size=1000)\n",
    "    .map(data_preparation._parse_image, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(params.BATCH_SIZE)\n",
    ")\n",
    "val_ds = (tf.data.Dataset.list_files(params.patches_dir + '/val/*/*')\n",
    "    .map(data_preparation._parse_image, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(params.BATCH_SIZE)\n",
    ")\n",
    "test_ds = (tf.data.Dataset.list_files(params.patches_dir + '/test/*/*')\n",
    "    .map(data_preparation._parse_image, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(params.BATCH_SIZE)\n",
    ")\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'],\n",
    "              experimental_run_tf_function=False)\n",
    "model.build(input_shape=[None, 256, 256, 1])\n",
    "constrain_conv_layer = constrain_conv(model)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "ckpts_callback = tf.keras.callbacks.ModelCheckpoint(filepath='./ckpts/dense/',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 monitor='val_accuracy', mode='max',\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)\n",
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [layer.name for layer in model.layers \n",
    "        if 'flipout' in layer.name]\n",
    "# dense_flipout/kernel_posterior_loc:0\n",
    "qm_vals = [layer.kernel_posterior.mean() \n",
    "        for layer in model.layers\n",
    "        if 'flipout' in layer.name]\n",
    "# this stddev is after softplus\n",
    "qs_vals = [layer.kernel_posterior.stddev()\n",
    "        for layer in model.layers\n",
    "        if 'flipout' in layer.name]\n",
    "\n",
    "utils.plot_weight_posteriors(names, qm_vals, qs_vals, fname=\"weight.png\")\n",
    "print(\"mean of mean is {}, mean variance is {}\".\n",
    "      format(tf.reduce_mean(qm_vals[0]),\n",
    "      tf.reduce_mean(qs_vals[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, epochs=10, \n",
    "                    callbacks=[constrain_conv_layer, ckpts_callback, tensorboard_callback], \n",
    "                    validation_data=val_ds, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
