{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import params\n",
    "import utils\n",
    "import os\n",
    "import tensorflow_probability as tfp\n",
    "import data_preparation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "tfd = tfp.distributions\n",
    "keras = tf.keras\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "NUM_CLASSES = len(params.brand_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class constrain_conv(tf.keras.models.Model, tf.keras.callbacks.Callback):\n",
    "    def __init__(self, model):\n",
    "        super(constrain_conv, self).__init__()\n",
    "        self.layer = model.layers[0]\n",
    "        self.pre_weights = None\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        weights = self.layer.get_weights()[0]\n",
    "        bias = self.layer.get_weights()[1]\n",
    "        if self.pre_weights is None or np.all(self.pre_weights != weights):\n",
    "            weights = weights*10000\n",
    "            weights[2, 2, :, :] = 0\n",
    "            s = np.sum(weights, axis=(0,1))\n",
    "            for i in range(3):\n",
    "                weights[:, :, 0, i] /= s[0, i]\n",
    "            weights[2, 2, :, :] = -1\n",
    "            self.pre_weights = weights\n",
    "        self.layer.set_weights([weights, bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparation.collect_split_extract(parent_dir=params.patches_dir, \n",
    "                                       download_images=False)\n",
    "\n",
    "train_size = 0\n",
    "val_size = 0\n",
    "num_images_per_class = []\n",
    "class_weight = {}\n",
    "for m in params.brand_models:\n",
    "    num_images = len(os.listdir(os.path.join(params.patches_dir, 'train', m)))\n",
    "    num_images_per_class.append(num_images)\n",
    "    train_size += num_images\n",
    "    val_size += len(os.listdir(os.path.join(params.patches_dir, 'val', m)))\n",
    "    \n",
    "num_batches = (train_size + params.BATCH_SIZE - 1) // params.BATCH_SIZE\n",
    "\n",
    "for n in range(len(params.brand_models)):\n",
    "    class_weight[n] = (1 / num_images_per_class[n])*(train_size)/2.0\n",
    "    print('Weight for class {}: {:.2f}'.format(n, class_weight[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def make_divergence_fn_for_empirical_bayes(std_prior_scale, examples_per_epoch):\n",
    "#     def divergence_fn(q, p, _):\n",
    "#         log_probs = tfd.LogNormal(0., std_prior_scale).log_prob(p.stddev())\n",
    "#         out = tfd.kl_divergence(q, p) - tf.reduce_sum(log_probs)\n",
    "#         return out / examples_per_epoch\n",
    "#     return divergence_fn\n",
    "\n",
    "\n",
    "# def make_prior_fn_for_empirical_bayes(init_scale_mean=-1, init_scale_std=0.1):\n",
    "#     \"\"\"Returns a prior function with stateful parameters for EB models.\"\"\"\n",
    "#     def prior_fn(dtype, shape, name, _, add_variable_fn):\n",
    "#         \"\"\"A prior for the variational layers.\"\"\"\n",
    "#         untransformed_scale = add_variable_fn(\n",
    "#             name=name + '_untransformed_scale',\n",
    "#             shape=(1,),\n",
    "#             initializer=tf.compat.v1.initializers.random_normal(\n",
    "#                 mean=init_scale_mean, stddev=init_scale_std),\n",
    "#             dtype=dtype,\n",
    "#             trainable=False)\n",
    "#         loc = add_variable_fn(\n",
    "#             name=name + '_loc',\n",
    "#             initializer=keras.initializers.Zeros(),\n",
    "#             shape=shape,\n",
    "#             dtype=dtype,\n",
    "#             trainable=True)\n",
    "#         # ??? why 1e-6 ???\n",
    "#         scale = 1e-6 + tf.nn.softplus(untransformed_scale)\n",
    "#         dist = tfd.Normal(loc=loc, scale=scale)\n",
    "#         batch_ndims = tf.size(input=dist.batch_shape_tensor())\n",
    "#         return tfd.Independent(dist, reinterpreted_batch_ndims=batch_ndims)\n",
    "#     return prior_fn\n",
    "\n",
    "# init_prior_scale_mean=-1.9994,\n",
    "# init_prior_scale_std=-0.30840,\n",
    "# std_prior_scale=3.4210\n",
    "\n",
    "# eb_prior_fn = make_prior_fn_for_empirical_bayes(\n",
    "#               init_prior_scale_mean, init_prior_scale_std)\n",
    "\n",
    "# divergence_fn = make_divergence_fn_for_empirical_bayes(\n",
    "#         std_prior_scale, train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divergence_fn = (lambda q, p, _: 0 * tfd.kl_divergence(q, p) /  # pylint: disable=g-long-lambda\n",
    "                        tf.cast(train_size, dtype=tf.float32))\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(3, (5, 5), \n",
    "                        padding='same'),\n",
    "    tfp.layers.Convolution2DFlipout(96, \n",
    "        kernel_size=7, strides=2, padding='SAME', \n",
    "        # kernel_prior_fn=eb_prior_fn,\n",
    "        kernel_divergence_fn=divergence_fn,\n",
    "        activation=tf.nn.selu),\n",
    "    tf.keras.layers.MaxPool2D(\n",
    "        pool_size=[3, 3], strides=2,\n",
    "        padding='SAME'),\n",
    "    tfp.layers.Convolution2DFlipout(\n",
    "        64, kernel_size=5, strides=1,\n",
    "        padding='SAME', \n",
    "        # kernel_prior_fn=eb_prior_fn,\n",
    "        kernel_divergence_fn=divergence_fn,\n",
    "        activation=tf.nn.selu),\n",
    "    tf.keras.layers.MaxPool2D(\n",
    "        pool_size=[3, 3], strides=2),\n",
    "    tfp.layers.Convolution2DFlipout(\n",
    "        64, kernel_size=5, strides=1,\n",
    "        padding='SAME', \n",
    "        # kernel_prior_fn=eb_prior_fn,\n",
    "        kernel_divergence_fn=divergence_fn,\n",
    "        activation=tf.nn.selu),\n",
    "    tf.keras.layers.MaxPool2D(\n",
    "        pool_size=[3, 3], strides=2),\n",
    "    tfp.layers.Convolution2DFlipout(\n",
    "        128, kernel_size=1, strides=1,\n",
    "        padding='SAME', \n",
    "        # kernel_prior_fn=eb_prior_fn,\n",
    "        kernel_divergence_fn=divergence_fn,\n",
    "        activation=tf.nn.selu),\n",
    "    tf.keras.layers.MaxPool2D(\n",
    "        pool_size=[3, 3], strides=2,),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tfp.layers.DenseFlipout(\n",
    "        200, kernel_divergence_fn=divergence_fn,\n",
    "        activation=tf.nn.selu),\n",
    "    tfp.layers.DenseFlipout(\n",
    "        200, kernel_divergence_fn=divergence_fn,\n",
    "        activation=tf.nn.selu),\n",
    "    tfp.layers.DenseFlipout(\n",
    "        NUM_CLASSES, \n",
    "        # kernel_prior_fn=eb_prior_fn,\n",
    "        kernel_divergence_fn=divergence_fn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = (tf.data.Dataset.list_files(params.patches_dir + '/train/*/*')\n",
    "    .shuffle(buffer_size=1000)\n",
    "    .map(data_preparation._parse_image, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(params.BATCH_SIZE))\n",
    "val_ds = (tf.data.Dataset.list_files(params.patches_dir + '/val/*/*')\n",
    "    .map(data_preparation._parse_image, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(params.BATCH_SIZE))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy', kl_loss, nll_loss],\n",
    "              experimental_run_tf_function=False)\n",
    "model.build(input_shape=[None, 256, 256, 1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constrain_conv_layer = constrain_conv(model)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "ckpts_callback = tf.keras.callbacks.ModelCheckpoint(filepath='./ckpts/5_num_batches/',\n",
    "                                                save_weights_only=True,\n",
    "                                                monitor='val_accuracy', mode='max',\n",
    "                                                save_best_only=True,\n",
    "                                                verbose=1)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                    # Stop training when `val_loss` is no longer improving\n",
    "                    monitor=\"val_loss\",\n",
    "                    # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "                    min_delta=1e-2,\n",
    "                    # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "                    patience=2,\n",
    "                    verbose=1,)\n",
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, epochs=10, \n",
    "                    callbacks=[constrain_conv_layer, ckpts_callback, tensorboard_callback, early_stopping], \n",
    "                    validation_data=val_ds, class_weight=class_weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
